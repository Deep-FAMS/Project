{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "import dotenv\n",
    "import io\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from IPython.display import Markdown, display\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "WORK = os.environ[\"WORK\"]\n",
    "PROJ_DIR = f'{WORK}/ADA_Project'\n",
    "\n",
    "def generate_latest_fakes_report(PROJ_DIR, exclude:list=None, verbose=False, export=False, display_output=False):\n",
    "\n",
    "    tr = f'{WORK}/ADA_Project/training_runs'\n",
    "\n",
    "    fid_logs = {}\n",
    "\n",
    "    tf_folders = glob(f'{tr}/*')\n",
    "    print(tf_folders)\n",
    "    if exclude is not None:\n",
    "        exclude = [x + '_training-runs' for x in exclude]\n",
    "        tf_folders = [x for x in tf_folders if Path(x).name not in exclude]\n",
    "    \n",
    "    for f in tf_folders:\n",
    "        folder = sorted(glob(f'{f}/*'))[-1]\n",
    "        fid_file = glob(f'{folder}/metric-*.txt')\n",
    "        if fid_file != []:\n",
    "            fid_file = fid_file[0]\n",
    "            dataset = Path(fid_file).parents[1].name.replace(\n",
    "                '_training-runs', '')\n",
    "            fid_logs[dataset] = fid_file\n",
    "\n",
    "    fid_logs['AFHQ-CAT'] = fid_logs.pop('AFHQ')\n",
    "\n",
    "#     fid_logs = {k.replace('FFHQ', 'FFHQ_custom'): v for k, v in fid_logs.items()}\n",
    "\n",
    "    findWholeWord = lambda w, s: re.compile(rf'\\b({w})\\b', flags=re.IGNORECASE\n",
    "                                            ).search(s)\n",
    "\n",
    "    snapshots = {}\n",
    "\n",
    "    for k, v in fid_logs.items():\n",
    "        with open(v) as f:\n",
    "            lines = f.readlines()\n",
    "            snapshots[k] = {}\n",
    "            snapshots[k]['scores'] = []\n",
    "            for line in lines:\n",
    "                if 'StyleGAN2' in k:\n",
    "                    string = 'fid50k'\n",
    "                else:\n",
    "                    string = 'fid50k_full'\n",
    "                sp = findWholeWord(string, line).span()\n",
    "                snapshot = line[:23]\n",
    "                score = float(line[sp[-1] + 1:sp[-1] + 7])\n",
    "                snapshots[k]['scores'].append({f'{snapshot}': score})\n",
    "\n",
    "    best_snapshots = {}\n",
    "\n",
    "    for ds in snapshots:\n",
    "        d = snapshots[ds]['scores']\n",
    "        keys = [list(x.keys()) for x in d]\n",
    "        vals = [list(x.values()) for x in d]\n",
    "        best_snapshots[ds] = {\n",
    "            'snapshot': keys[vals.index(min(vals))][0],\n",
    "            'score': min(vals)[0]\n",
    "        }\n",
    "\n",
    "    files = [v.replace('metric-fid50k_full.txt', 'log.txt').replace(\n",
    "        'metric-fid50k.txt', 'log.txt')\n",
    "             for k, v in fid_logs.items()]\n",
    "\n",
    "    for (k, v), f in zip(best_snapshots.items(), files):\n",
    "        best_snapshots[k]['file'] = f.replace(f'{tr}/' , '')\n",
    "\n",
    "    if export is True:\n",
    "        with open(f'{PROJ_DIR}/FID_of_best_snapshots.json', 'w') as out_file:\n",
    "            json.dump(best_snapshots, out_file, indent=4)\n",
    "\n",
    "            \n",
    "    d = best_snapshots\n",
    "\n",
    "    for k, v in best_snapshots.items():\n",
    "        d[k]['training_time'] = []\n",
    "\n",
    "\n",
    "    findWholeWord = lambda w, s: re.compile(rf'\\b({w})\\b', flags=re.IGNORECASE\n",
    "                                            ).search(s)\n",
    "\n",
    "    def calc_time(t, unit):\n",
    "        s = t.partition(unit)[0][-2:].replace(' ', '')\n",
    "        if t.partition(unit)[0] != t:\n",
    "            return int(s)\n",
    "        return 0\n",
    "\n",
    "    TTs = {}\n",
    "\n",
    "    for k, v in d.items():\n",
    "        file = f'{tr}/{d[k][\"file\"]}'\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        if 'Exporting sample images...' in lines[-1]:\n",
    "            continue\n",
    "        else:\n",
    "            snap = f'{tr}/{d[k][\"snapshot\"]}'\n",
    "            snap = Path(snap).name\n",
    "            for line in lines:\n",
    "                if snap in line:\n",
    "                    line_idx = lines.index(line) - 2\n",
    "                    line = lines[line_idx]\n",
    "                    try:\n",
    "                        sp = findWholeWord('time', line).span()\n",
    "                        t = line[sp[1] + 1:sp[1] + 12]\n",
    "                        last = t.partition('s')[-1]\n",
    "                        t = t.replace(last, '')\n",
    "                        T = (calc_time(t, 'd') * 24) + calc_time(t, 'h') + (\n",
    "                            calc_time(t, 'm') / 60) + (calc_time(t, 's') / 3600)\n",
    "                        d[k]['training_time'].append(T)\n",
    "\n",
    "                    except AttributeError:\n",
    "                        continue\n",
    "\n",
    "    days = [round(j['training_time'][0] / 24, 1) for i, j in d.items()]\n",
    "\n",
    "    table = tabulate([[x, round(y['training_time'][0], 2), z, round(y['score'], 2)]\n",
    "                      for (x, y), z in zip(d.items(), days)],\n",
    "                     headers=[\n",
    "                         'Dataset', 'Training time (in hrs)',\n",
    "                         'Training time (in days)', 'FID'\n",
    "                     ],\n",
    "                     tablefmt='github')\n",
    "\n",
    "\n",
    "\n",
    "    def upload_img(image, token):\n",
    "        with open(image, \"rb\") as file:\n",
    "            url = \"https://api.imgbb.com/1/upload\"\n",
    "            parameters = {\n",
    "                \"key\": token,\n",
    "                \"image\": base64.b64encode(file.read()),\n",
    "            }\n",
    "            res = requests.post(url, parameters)\n",
    "            link = res.json()\n",
    "            url = link['data']['url']\n",
    "            return url\n",
    "\n",
    "    dotenv.load_dotenv(f'{PROJ_DIR}/.env')\n",
    "    token = os.getenv('TOKEN')\n",
    "\n",
    "    mb_size = lambda x: Path(x).stat().st_size / (1024 * 1024)\n",
    "    dir_up = lambda x, y: \"/\".join(Path(x).parts[y:])\n",
    "\n",
    "    TRfolders_ = f'{PROJ_DIR}/training_runs'\n",
    "    TRfolders = glob(f'{TRfolders_}/*')\n",
    "    backups_dir = f'{PROJ_DIR}/.tmp_imgs'\n",
    "    Path(backups_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    md_content = []\n",
    "    latest_fakes = [str(Path(tr + '/' + d[k][\"file\"]).parent) +\n",
    "                    f'/{d[k][\"snapshot\"]}.png'.replace('network-snapshot-', 'fakes')\n",
    "                    for k, v in d.items()]\n",
    "\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime('%m/%d/%Y, %H:%M:%S')\n",
    "    md_content.append('# Latest fakes\\n')\n",
    "    md_content.append(f'## Date and time: {date_time}\\n')\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print('=' * 90, '\\n\\nLatest fakes:\\n')\n",
    "        pprint([x.replace(str(TRfolders_), '') for x in latest_fakes])\n",
    "        print('\\n', '=' * 90, '\\n')\n",
    "\n",
    "    for img in latest_fakes:\n",
    "        image = Image.open(img)\n",
    "        compressed_path = f'{backups_dir}/{Path(img).stem}' + '.jpg'\n",
    "        \n",
    "        if 'StyleGAN2_WILD-AFHQ' in img:\n",
    "            left, top, right, bottom = 0, 0, 256 * 15, 256 * 8\n",
    "            image = image.crop((left, top, right, bottom))\n",
    "            temp = io.BytesIO()\n",
    "            \n",
    "        image.save(compressed_path)\n",
    "            \n",
    "        if verbose:\n",
    "            print(\n",
    "                Path(img).name,\n",
    "                f'compressed from ({mb_size(img):.2f}MB) to ==> '\n",
    "                f'({mb_size(compressed_path):.2f}MB)')\n",
    "\n",
    "        url = upload_img(compressed_path, token)\n",
    "        if verbose == 1:\n",
    "            print(f'Link ==> {url}\\n')\n",
    "        img_subdir = dir_up(img, -3)\n",
    "\n",
    "        md_content.append(\n",
    "            f'### {img_subdir}\\n'\n",
    "            f'![{Path(compressed_path).name}]({url} \"{img_subdir}\")'\n",
    "            '\\n\\n')\n",
    "\n",
    "#     Tstamp = datetime.now().strftime('%m_%d_%Y__%H_%M')\n",
    "    report_path = f'{PROJ_DIR}/latest_fakes_report.md'\n",
    "\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(''.join(md_content))\n",
    "        f.write(table)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Generated a report at ==> {report_path}')\n",
    "\n",
    "    if display_output:\n",
    "        display(Markdown(report_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_latest_fakes_report(PROJ_DIR=PROJ_DIR, verbose=True, export=True, display_output=True,\n",
    "                             exclude=['POKEMON', 'ANIME-FACES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ada-env)",
   "language": "python",
   "name": "ada-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
